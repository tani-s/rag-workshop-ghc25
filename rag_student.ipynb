{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4471cb49",
   "metadata": {},
   "source": [
    "# ğŸ§  Study Buddy â€” RAG Student Notebook\n",
    "Fill in the missing pieces to complete your own RAG pipeline using Gemini.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1cbddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Step 1: Install dependencies\n",
    "!pip install -q google-generativeai PyPDF2 faiss-cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04355112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Step 2: Import libraries\n",
    "import google.generativeai as genai\n",
    "from getpass import getpass\n",
    "import PyPDF2\n",
    "import faiss\n",
    "import numpy as np\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66903e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Step 3: Configure Gemini API\n",
    "# ğŸ‘‰ Fill in the variable name if needed\n",
    "GEMINI_API_KEY = getpass(\"ğŸ”‘ Enter your Gemini API key: \")\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b2a767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Step 4: Upload a PDF or text file\n",
    "# (This code is complete for you.)\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "\n",
    "file_name = list(uploaded.keys())[0]\n",
    "text = \"\"\n",
    "\n",
    "if file_name.endswith(\".pdf\"):\n",
    "    reader = PyPDF2.PdfReader(file_name)\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text() or \"\"\n",
    "else:\n",
    "    text = uploaded[file_name].decode(\"utf-8\")\n",
    "\n",
    "print(f\"âœ… Loaded {len(text)} characters from {file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151f5909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Step 5: Chunking\n",
    "# We provide the chunking logic. You choose the chunk_size if you want.\n",
    "\n",
    "def split_text(text, chunk_size=1000, overlap=200):\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunks.append(text[start:end])\n",
    "        start += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "chunks = split_text(text)\n",
    "print(f\"ğŸ“š Split into {len(chunks)} chunks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2db6499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Step 6: Create embeddings + vector index\n",
    "# â— Student TODO:\n",
    "# 1. Set the embed model name\n",
    "# 2. Call genai.embed_content\n",
    "# 3. Build the FAISS index\n",
    "\n",
    "embed_model = \"<<< FILL IN MODEL NAME >>>\"  # e.g., \"models/gemini-embedding-001\"\n",
    "\n",
    "embeddings = []\n",
    "\n",
    "for chunk in chunks:\n",
    "    # ğŸ‘‰ TODO: generate an embedding for each chunk\n",
    "    result = genai.embed_content(model=embed_model, content=chunk)\n",
    "    embeddings.append( result[\"embedding\"] )\n",
    "\n",
    "embeddings = np.array(embeddings, dtype=\"float32\")\n",
    "\n",
    "# ğŸ‘‰ TODO: Initialize a FAISS index\n",
    "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "index.add(embeddings)\n",
    "\n",
    "print(\"âœ… Vector index built!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a668669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Step 7: RAG Retrieval + Answering\n",
    "# â— Student TODO:\n",
    "# 1. Write the retrieve(query) function\n",
    "# 2. Write the ask_study_buddy(query) function\n",
    "\n",
    "def retrieve(query, k=3):\n",
    "    # ğŸ‘‰ TODO: embed the query\n",
    "    q_embed = genai.embed_content(model=embed_model, content=query)[\"embedding\"]\n",
    "    _, idx = index.search(np.array([q_embed], dtype=\"float32\"), k)\n",
    "    return [chunks[i] for i in idx[0]]\n",
    "\n",
    "def ask_study_buddy(query):\n",
    "    docs = retrieve(query)\n",
    "    context = \"\\n\\n\".join(docs)\n",
    "\n",
    "    # ğŸ‘‰ TODO: create your prompt template\n",
    "    prompt = f\"\"\"\n",
    "You are Study Buddy, a helpful learning assistant.\n",
    "Use the context to answer the question clearly.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\"\"\"\n",
    "\n",
    "    model = genai.GenerativeModel(\"gemini-2.5-flash\")\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926fc1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Step 8: Test!\n",
    "question = \"<<< TYPE YOUR QUESTION HERE >>>\"\n",
    "print(\"ğŸ¤” Q:\", question)\n",
    "print(\"ğŸ’¡ A:\", ask_study_buddy(question))\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
