{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c255f8e7",
      "metadata": {
        "id": "c255f8e7"
      },
      "source": [
        "# ğŸ§  Study Buddy â€” Build Your Own RAG Chatbot with Gemini\n",
        "You'll complete just the key parts of a RAG pipeline:\n",
        "\n",
        "âœ… retrieval (FAISS search)\n",
        "âœ… building the prompt\n",
        "âœ… calling the LLM\n",
        "âœ… testing your RAG chatbot\n",
        "\n",
        "Everything else is already handled for you.\n"
        "Upload any (relatively long ~10 page) PDF or text file (e.g., course notes, a Wikipedia export, or an article).\n",
        "\n",
        "Ask questions like:\n",
        "- â€œSummarize Chapter 2â€\n",
        "- â€œWhat is reinforcement learning?â€\n",
        "- â€œWhatâ€™s the main takeaway from the section about ...?â€\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1359b2c9",
      "metadata": {
        "id": "1359b2c9"
      },
      "outputs": [],
      "source": [
        "# ğŸ§© Step 1: Install dependencies\n",
        "!pip install -q google-generativeai PyPDF2 faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12ff5e77",
      "metadata": {
        "id": "12ff5e77"
      },
      "outputs": [],
      "source": [
        "# ğŸ§  Step 2: Import libraries\n",
        "import google.generativeai as genai\n",
        "from getpass import getpass\n",
        "import PyPDF2\n",
        "import faiss\n",
        "import numpy as np\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5777121e",
      "metadata": {
        "id": "5777121e"
      },
      "outputs": [],
      "source": [
        "# âš™ï¸ Step 3: Configure Gemini API\n",
        "GEMINI_API_KEY = getpass(\"ğŸ”‘ Enter your Gemini API key: \")\n",
        "genai.configure(api_key=GEMINI_API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1dd7792f",
      "metadata": {
        "id": "1dd7792f"
      },
      "outputs": [],
      "source": [
        "# ğŸ§¾ Step 4: Upload your study material\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "file_name = list(uploaded.keys())[0]\n",
        "text = \"\"\n",
        "\n",
        "if file_name.endswith(\".pdf\"):\n",
        "    reader = PyPDF2.PdfReader(file_name)\n",
        "    for page in reader.pages:\n",
        "        text += page.extract_text() or \"\"\n",
        "else:\n",
        "    text = uploaded[file_name].decode(\"utf-8\")\n",
        "\n",
        "print(f\"âœ… Loaded {len(text)} characters from {file_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9256317a",
      "metadata": {
        "id": "9256317a"
      },
      "outputs": [],
      "source": [
        "# ğŸª„ Step 5: Split text into chunks\n",
        "def split_text(text, chunk_size=1000, overlap=200):\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    while start < len(text):\n",
        "        end = start + chunk_size\n",
        "        chunks.append(text[start:end])\n",
        "        start += chunk_size - overlap\n",
        "    return chunks\n",
        "\n",
        "chunks = split_text(text)\n",
        "print(f\"ğŸ“š Split into {len(chunks)} chunks\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b395fbce",
      "metadata": {
        "id": "b395fbce"
      },
      "outputs": [],
      "source": [
        "# ğŸ§© Step 6: Create embeddings and index\n",
        "embed_model = \"models/gemini-embedding-001\"\n",
        "embeddings = []\n",
        "\n",
        "for chunk in chunks:\n",
        "    result = genai.embed_content(model=embed_model, content=chunk)\n",
        "    embeddings.append(result[\"embedding\"])\n",
        "\n",
        "embeddings = np.array(embeddings, dtype=\"float32\")\n",
        "\n",
        "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
        "index.add(embeddings)\n",
        "print(\"âœ… Vector index built!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# âœ… Step 7: Fill in the Retrieval + RAG Logic\n",
        "You're writing the MOST important part of a RAG system:\n",
        "\n",
        "1. Embed the user query\n",
        "2. Search the FAISS vector index\n",
        "3. Retrieve the top-k chunks\n",
        "4. Build the prompt\n",
        "5. Call the LLM\n",
        "\n",
        "Fill in the `YOUR CODE HERE` parts only.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve(query, k=3):\n",
        "    # ğŸ‘‰ TODO: Embed the query\n",
        "    # q_embed = genai.embed_content(...)\n",
        "\n",
        "    # ğŸ‘‰ TODO: FAISS similarity search\n",
        "    # _, idx = index.search(...)\n",
        "\n",
        "    # ğŸ‘‰ TODO: return the matching chunks\n",
        "    # return [chunks[i] for i in idx[0]]\n",
        "    pass\n",
        "\n",
        "\n",
        "def ask_rag(query):\n",
        "    docs = retrieve(query)\n",
        "    context = \"\\n\\n\".join(docs)\n",
        "\n",
        "    # ğŸ‘‰ TODO: Write a good RAG prompt\n",
        "    prompt = f\"YOUR PROMPT HERE\"\n",
        "\n",
        "    # ğŸ‘‰ TODO: call the Gemini LLM\n",
        "    # model = genai.GenerativeModel(\"gemini-2.5-flash\")\n",
        "    # response = model.generate_content(prompt)\n",
        "\n",
        "    # ğŸ‘‰ TODO: return model output\n",
        "    # return response.text\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# âœ… Step 8: Run your RAG chatbot!\n",
        "Put any question you want here.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"YOUR QUESTION HERE\"\n",
        "print(\"ğŸ¤” Q:\", question)\n",
        "\n",
        "# ğŸ‘‰ TODO: call ask_rag()\n",
        "# print(ask_rag(question))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
